{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neerajkumarkannoujiya/11_JavaSpark/blob/main/Neeraj_copy_of_Final_version_BDA_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DovdjuI9vFnP",
        "outputId": "87f37390-b07e-40db-a36e-094bbe7f317e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connected to r2u.\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [73.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,161 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,101 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,694 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,836 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Fetched 24.9 MB in 3s (8,316 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GWmgSfZvTS4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6g1TzfUvcSy"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqwSlyWnvdpY"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13m-D-h-vfNG"
      },
      "outputs": [],
      "source": [
        "!tar -xzvf hadoop-3.3.4.tar.gz && cp -r hadoop-3.3.4/ /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Fj0t7QRLvgcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e54422-4377-4b6c-cdc2-89b9d8f12ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: /home/neeraj7388011/hadoop/etc/hadoop/hadoop-env.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#To find the default Java path and add export in hadoop-env.sh\n",
        "JAVA_HOME = !readlink -f /usr/lib/jvm/java-11-openjdk-amd64 | sed \"s:bin/java::\"\n",
        "java_home_text_command = f\"$ {JAVA_HOME[0]} \"\n",
        "!echo export JAVA_HOME=$java_home_text >>/home/neeraj7388011/hadoop/etc/hadoop/hadoop-env.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhiWmykWvhqb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME']=\"/home/neeraj7388011/hadoop\"\n",
        "os.environ['JAVA_HOME']=java_home_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4pcMM41vi6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38bfad83-495a-46eb-a3c5-4782c5b39ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: /home/neeraj7388011/hadoop/bin/hadoop: No such file or directory\n",
            "/bin/bash: line 1: /home/neeraj7388011/hadoop/bin/hadoop: No such file or directory\n",
            "/bin/bash: line 1: /home/neeraj7388011/hadoop/bin/hadoop: No such file or directory\n",
            "/bin/bash: line 1: /home/neeraj7388011/hadoop/bin/hadoop: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!/home/neeraj7388011/hadoop/bin/hadoop\n",
        "!/home/neeraj7388011/hadoop/bin/hadoop fs -mkdir elec2_repo\n",
        "!/home/neeraj7388011/hadoop/bin/hadoop fs -copyFromLocal /content/elec2_* elec2_repo/\n",
        "!/home/neeraj7388011/hadoop/bin/hadoop fs -ls elec2_repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWvMz9Auvn1U"
      },
      "outputs": [],
      "source": [
        "! export HDFS_NAMENODE_USER=\"root\"\n",
        "! export HDFS_DATANODE_USER=\"root\"\n",
        "! export HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
        "! export YARN_RESOURCEMANAGER_USER=\"root\"\n",
        "! export YARN_NODEMANAGER_USER=\"root\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEx-TARKvozr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "02423ad7-7250-4d12-db79-00f50e8d5cba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/elec2_repo/elec2_data.dat.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-17c0285b7a83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ElectricityDataProcessing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"elec2_repo/elec2_data.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/elec2_repo/elec2_data.dat."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ElectricityDataProcessing\").getOrCreate()\n",
        "df = spark.read.option(\"delimiter\", \" \") .option(\"inferSchema\", \"true\").csv(\"elec2_repo/elec2_data.dat\", header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHjp1GVivp1s"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rr5naRXvrfd"
      },
      "outputs": [],
      "source": [
        "column_names = [\"date\", \"day\", \"period\", \"nswprice\", \"nswdemand\", \"vicprice\", \"vicdemand\", \"transfer\"]\n",
        "df = df.toDF(*column_names)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzdrHdyBvvEQ"
      },
      "outputs": [],
      "source": [
        "df_processed = df.drop(\"nswprice\", \"date\")\n",
        "df_processed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiBCiuuovxae"
      },
      "outputs": [],
      "source": [
        "df_labels = spark.read.option(\"delimiter\", \" \") .option(\"inferSchema\", \"true\").csv(\"elec2_repo/elec2_label.dat\", header=False)\n",
        "df_labels.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3-2Fp6Ev2ge"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-io\n",
        "!pip install kafka-python\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "# Download Kafka (double-check the URL to ensure it is correct)\n",
        "!curl -O https://downloads.apache.org/kafka/3.7.1/kafka_2.12-3.7.1.tgz\n",
        "\n",
        "# Extract the tarball (once the file is confirmed to be correct)\n",
        "!tar -xzf kafka_2.12-3.7.1.tgz\n",
        "\n",
        "!./kafka_2.12-3.7.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-3.7.1/config/zookeeper.properties\n",
        "!./kafka_2.12-3.7.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-3.7.1/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10\n",
        "!ps -ef | grep kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAfdqoazv4tz"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install kafka-python\n",
        "!pip install dash\n",
        "!pip install dash-bootstrap-components\n",
        "!pip install plotly\n",
        "!pip install confluent-kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKpK8o6_wEec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733fb9ec-3b10-471b-fdfe-28e91274822a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+--------+--------+-------------------+---------------------+------------------+\n",
            "|   date|day|  period|nswprice|          nswdemand|running_avg_nswdemand|next_day_nswdemand|\n",
            "+-------+---+--------+--------+-------------------+---------------------+------------------+\n",
            "|2.65E-4|  1|     0.0|0.047106|0.28772452910185575|             0.282654|          0.247694|\n",
            "|2.65E-4|  1|0.021277|0.047106| 0.2481024813385884|             0.265174|          0.233859|\n",
            "|2.65E-4|  1|0.042553|0.047106|0.24800524850405736|   0.2547356666666667|          0.185808|\n",
            "|2.65E-4|  1| 0.06383|0.046956|0.17573529925663744|           0.23750375|          0.154865|\n",
            "|2.65E-4|  1|0.085106|0.046956|0.16210930391250145|             0.220976|          0.127789|\n",
            "|2.65E-4|  1|0.106383|0.046956| 0.1285261792133506|  0.20544483333333333|          0.112169|\n",
            "|2.65E-4|  1| 0.12766|0.046956|0.13340348600026813|  0.19211971428571428|          0.101309|\n",
            "|2.65E-4|  1|0.148936|0.046806|0.08301315201076573|          0.180768375|           0.10235|\n",
            "|2.65E-4|  1|0.170213|0.046806|0.12233058361432436|  0.17205522222222222|          0.110533|\n",
            "|2.65E-4|  1|0.191489|0.046806|0.12312356430240316|             0.165903|          0.143112|\n",
            "|2.65E-4|  1|0.212766|0.046806| 0.1409179096897806|   0.1638310909090909|          0.187742|\n",
            "|2.65E-4|  1|0.234043|0.047256|    0.1953738250461|  0.15520272727272727|           0.27953|\n",
            "|2.65E-4|  1|0.255319|0.047256|0.27410215204923793|   0.1580969090909091|          0.367599|\n",
            "|2.65E-4|  1|0.276596|0.047106|0.38174880217513174|   0.1702550909090909|          0.375483|\n",
            "|2.65E-4|  1|0.297872| 0.10475|0.38603148091440537|  0.18749827272727274|          0.435882|\n",
            "|2.65E-4|  1|0.319149| 0.10475| 0.4453100397688422|  0.21304527272727272|          0.465338|\n",
            "|2.65E-4|  1|0.340426|0.155878|0.44474995290455493|  0.24373154545454545|          0.469652|\n",
            "|2.65E-4|  1|0.361702|0.153206|0.47521272251077656|              0.27623|          0.512348|\n",
            "|2.65E-4|  1|0.382979| 0.10475| 0.4942835149559415|  0.31359718181818186|          0.529753|\n",
            "|2.65E-4|  1|0.404255| 0.10475| 0.5441717124717099|             0.352452|          0.519042|\n",
            "+-------+---+--------+--------+-------------------+---------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df_processed = df.drop(\"vicprice\", \"vicdemand\", \"transfer\")\n",
        "\n",
        "# Create a running average column based on 'vicdemand'\n",
        "window_spec = Window.orderBy(\"day\").rowsBetween(-10, 0)  # Adjust window size as needed\n",
        "df_processed = df_processed.withColumn(\"running_avg_nswdemand\", F.avg(\"nswdemand\").over(window_spec))\n",
        "window_spec_next = Window.orderBy(\"day\")\n",
        "df_processed = df_processed.withColumn(\"next_day_nswdemand\", F.lead(\"nswdemand\", 1).over(window_spec_next))\n",
        "avg_nswdemand = df_processed.select(F.avg(\"nswdemand\")).first()[0]\n",
        "df_processed = df_processed.withColumn(\n",
        "    \"next_day_nswdemand\",\n",
        "    F.when(F.col(\"next_day_nswdemand\").isNull(), avg_nswdemand).otherwise(F.col(\"next_day_nswdemand\"))\n",
        ")\n",
        "noise_magnitude = 0.05 * avg_nswdemand\n",
        "df_processed = df_processed.withColumn(\n",
        "    \"nswdemand\",\n",
        "    F.col(\"nswdemand\") + (F.rand(seed=42) - 0.5) * 2 * noise_magnitude\n",
        ")\n",
        "df_processed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxeDxRc-LiS-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "62614b2e-3e29-447b-ac36-68b766ad1472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8052, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "import plotly.graph_objs as go\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import threading\n",
        "import time\n",
        "from datetime import datetime\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "import tensorflow_io as tfio\n",
        "import numpy as np\n",
        "from scipy.stats import wasserstein_distance, ks_2samp\n",
        "import random\n",
        "\n",
        "pre_trained_cnt, pre_trained_sum = 1,0\n",
        "online_cnt, online_sum = 1,0\n",
        "\n",
        "KAFKA_TOPIC = 'electricity_demand'\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import Row\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "import json\n",
        "\n",
        "class KafkaDataProducer:\n",
        "    def __init__(self, spark_df, pre_trained_model, online_model):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
        "        )\n",
        "\n",
        "        self.data = spark_df.toPandas()\n",
        "        self.current_index = 0\n",
        "        self.pre_trained_model = pre_trained_model\n",
        "        self.online_model = online_model\n",
        "\n",
        "    def start_producing(self):\n",
        "        def produce_data():\n",
        "            global pre_trained_cnt, pre_trained_sum, online_cnt, online_sum\n",
        "            while self.current_index < len(self.data):\n",
        "                row = self.data.iloc[self.current_index]\n",
        "                last_row = row\n",
        "                if self.current_index != 0:\n",
        "                    last_row = self.data.iloc[self.current_index - 1]\n",
        "\n",
        "                features_vector = Vectors.dense([float(last_row['nswdemand']),float(last_row['running_avg_nswdemand']), int(row['day']), float(row['period'])])\n",
        "\n",
        "                pandas_df = pd.DataFrame({'features': [features_vector], 'next_day_nswdemand': [float(row['nswdemand'])]})\n",
        "\n",
        "                spark_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "                batch_X = spark_df.select(\"features\").rdd.map(lambda row: row['features'].toArray()).collect()\n",
        "                pre_trained_prediction = self.pre_trained_model.predict(batch_X)[0]\n",
        "\n",
        "                online_prediction = self.online_model.predict(batch_X)[0]\n",
        "\n",
        "                batch_X = spark_df.select(\"features\").rdd.map(lambda row: row['features'].toArray()).collect()\n",
        "                batch_y = spark_df.select(\"next_day_nswdemand\").rdd.map(lambda row: row['next_day_nswdemand']).collect()\n",
        "                self.online_model.partial_fit(batch_X, batch_y)\n",
        "                if self.current_index != 0:\n",
        "                    pre_trained_cnt += 1\n",
        "                    online_cnt += 1\n",
        "                    pre_trained_sum += abs(float(last_row['nswdemand']) - float(pre_trained_prediction))\n",
        "                    online_sum += abs(float(last_row['nswdemand']) - float(online_prediction))\n",
        "\n",
        "                message = {\n",
        "                    'timestamp': self.current_index,\n",
        "                    'nswdemand': float(row['nswdemand']),\n",
        "                    'pre_trained_prediction': float(pre_trained_prediction),\n",
        "                    'online_prediction': float(online_prediction)\n",
        "                }\n",
        "\n",
        "                self.producer.send(KAFKA_TOPIC, message)\n",
        "                self.current_index += 1\n",
        "                time.sleep(0.05 + random.randint(1,10) / 1000)\n",
        "\n",
        "            self.producer.flush()\n",
        "            self.current_index = 0\n",
        "\n",
        "        thread = threading.Thread(target=produce_data)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "def predict(model, df_batch):\n",
        "    predictions = model.transform(df_batch)\n",
        "    return predictions.select(\"next_day_nswdemand\", \"prediction\")\n",
        "\n",
        "class RealtimeDashboard:\n",
        "    def __init__(self, pre_trained_model, online_model):\n",
        "\n",
        "        self.x_data = []\n",
        "        self.y_data = []\n",
        "        self.y_pre_trained_pred = []\n",
        "        self.y_online_pred = []\n",
        "        self.predicted_demand = []\n",
        "        self.abs_diff_sum_pre_trained = 0\n",
        "        self.abs_diff_sum_online = 0\n",
        "        self.DISPLAY_WINDOW = 200\n",
        "        self.DRIFT_WINDOW = 20\n",
        "        self.DRIFT_THRESHOLD = 2.0\n",
        "        self.KS_PVALUE_THRESHOLD = 0.2\n",
        "        self.WASSERSTEIN_THRESHOLD = 0.2\n",
        "\n",
        "        self.drift_points = []\n",
        "        self.drift_points_ks = []\n",
        "        self.drift_points_wDistance = []\n",
        "        self.y_pre_trained_pe = []\n",
        "        self.y_online_pe = []\n",
        "\n",
        "        self.consumer = KafkaConsumer(\n",
        "            KAFKA_TOPIC,\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "            auto_offset_reset='earliest'\n",
        "        )\n",
        "\n",
        "        self.app = dash.Dash(__name__)\n",
        "        self.create_layout()\n",
        "        self.setup_callbacks()\n",
        "\n",
        "        self.pre_trained_model = pre_trained_model\n",
        "        self.online_model = online_model\n",
        "\n",
        "        self.start_consuming()\n",
        "\n",
        "    def start_consuming(self):\n",
        "        def consume_data():\n",
        "            previous_nswdemand = None\n",
        "            for message in self.consumer:\n",
        "                data = message.value\n",
        "                current_nswdemand = data['nswdemand']\n",
        "\n",
        "                # Calculate Percentage Error using the previous demand and current predictions\n",
        "                if previous_nswdemand is not None:\n",
        "                    pre_trained_pe = abs((previous_nswdemand - data['pre_trained_prediction']) / previous_nswdemand) * 100\n",
        "                    online_pe = abs((previous_nswdemand - data['online_prediction']) / previous_nswdemand) * 100\n",
        "\n",
        "                    self.y_pre_trained_pe.append(pre_trained_pe)\n",
        "                    self.y_online_pe.append(online_pe)\n",
        "\n",
        "                self.x_data.append(data['timestamp'])\n",
        "                self.y_data.append(current_nswdemand)\n",
        "                self.y_pre_trained_pred.append(data['pre_trained_prediction'])\n",
        "                self.y_online_pred.append(data['online_prediction'])\n",
        "\n",
        "                self.detect_drift()\n",
        "                self.detect_drift_ks()\n",
        "                self.detect_drift_wDistance()\n",
        "\n",
        "                # Use the previous demand for deviation calculations\n",
        "                if previous_nswdemand is not None:\n",
        "                    self.abs_diff_sum_pre_trained += abs(previous_nswdemand - data['pre_trained_prediction'])\n",
        "                    self.abs_diff_sum_online += abs(previous_nswdemand - data['online_prediction'])\n",
        "\n",
        "                if len(self.x_data) > self.DISPLAY_WINDOW:\n",
        "                    self.x_data = self.x_data[-self.DISPLAY_WINDOW:]\n",
        "                    self.y_data = self.y_data[-self.DISPLAY_WINDOW:]\n",
        "                    self.y_pre_trained_pred = self.y_pre_trained_pred[-self.DISPLAY_WINDOW:]\n",
        "                    self.y_online_pred = self.y_online_pred[-self.DISPLAY_WINDOW:]\n",
        "                    self.y_pre_trained_pe = self.y_pre_trained_pe[-self.DISPLAY_WINDOW:]\n",
        "                    self.y_online_pe = self.y_online_pe[-self.DISPLAY_WINDOW:]\n",
        "\n",
        "                previous_nswdemand = current_nswdemand\n",
        "\n",
        "        thread = threading.Thread(target=consume_data)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "\n",
        "    def create_layout(self):\n",
        "\n",
        "        self.app.layout = html.Div([\n",
        "            html.H1(\"Real-time Electricity Demand Dashboard\",\n",
        "                    style={'textAlign': 'center', 'color': '#2c3e50', 'marginBottom': 20}),\n",
        "\n",
        "            html.Div([\n",
        "                dcc.Graph(id='live-graph', animate=True),\n",
        "                dcc.Graph(id='stats-graph', animate=True),\n",
        "            ], style={'display': 'flex', 'flexDirection': 'row'}),\n",
        "\n",
        "            html.Div([\n",
        "                dcc.Graph(id='demand-with-predictions', animate=True),\n",
        "            ], style={'marginTop': 20}),\n",
        "            html.Div([\n",
        "                dcc.Graph(id='percentage-error-graph', animate=True),\n",
        "            ], style={'marginTop': 20}),\n",
        "            html.Div([\n",
        "                html.P(id='pre-trained-deviation'),\n",
        "                html.P(id='online-deviation'),\n",
        "            ]),\n",
        "\n",
        "            dcc.Interval(\n",
        "                id='graph-update',\n",
        "                interval=1000,\n",
        "                n_intervals=0\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def detect_drift_ks(self):\n",
        "        if len(self.y_data) >= self.DRIFT_WINDOW * 2:\n",
        "\n",
        "            prev_window = self.y_data[-2*self.DRIFT_WINDOW:-self.DRIFT_WINDOW]\n",
        "            curr_window = self.y_data[-self.DRIFT_WINDOW:]\n",
        "\n",
        "            ks_stat, ks_p_value = ks_2samp(prev_window, curr_window)\n",
        "\n",
        "            drift_detected = False\n",
        "\n",
        "            if ks_p_value < self.KS_PVALUE_THRESHOLD:\n",
        "                drift_detected = True\n",
        "\n",
        "            if drift_detected:\n",
        "\n",
        "                if len(self.drift_points_ks) == 0 or abs(self.drift_points_ks[-1] - self.x_data[-1]) >= 20:\n",
        "                    self.drift_points_ks.append(self.x_data[-1])\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "    def detect_drift_wDistance(self):\n",
        "        if len(self.y_data) >= self.DRIFT_WINDOW * 2:\n",
        "\n",
        "            prev_window = self.y_data[-2*self.DRIFT_WINDOW:-self.DRIFT_WINDOW]\n",
        "            curr_window = self.y_data[-self.DRIFT_WINDOW:]\n",
        "\n",
        "            wasserstein_dist = wasserstein_distance(prev_window, curr_window)\n",
        "\n",
        "            drift_detected = False\n",
        "\n",
        "            if wasserstein_dist > self.WASSERSTEIN_THRESHOLD:\n",
        "                drift_detected = True\n",
        "\n",
        "            if drift_detected:\n",
        "\n",
        "                if len(self.drift_points_wDistance) == 0 or abs(self.drift_points_wDistance[-1] - self.x_data[-1]) >= 20:\n",
        "                    self.drift_points_wDistance.append(self.x_data[-1])\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def detect_drift(self):\n",
        "        if len(self.y_data) >= self.DRIFT_WINDOW * 2:\n",
        "            prev_window = self.y_data[-2*self.DRIFT_WINDOW:-self.DRIFT_WINDOW]\n",
        "            curr_window = self.y_data[-self.DRIFT_WINDOW:]\n",
        "\n",
        "            wasserstein_dist = wasserstein_distance(prev_window, curr_window)\n",
        "            ks_stat, ks_p_value = ks_2samp(prev_window, curr_window)\n",
        "\n",
        "            drift_detected = False\n",
        "            if wasserstein_dist > self.WASSERSTEIN_THRESHOLD:\n",
        "                drift_detected = True\n",
        "            if ks_p_value < self.KS_PVALUE_THRESHOLD:\n",
        "                drift_detected = True\n",
        "\n",
        "            if drift_detected:\n",
        "                if len(self.drift_points) == 0 or abs(self.drift_points[-1] - self.x_data[-1]) >= 20:\n",
        "                    self.drift_points.append(self.x_data[-1])\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def calculate_statistics(self):\n",
        "        if len(self.y_data) >= 10:\n",
        "            window_size = 10\n",
        "            rolling_mean = pd.Series(self.y_data).rolling(window=window_size).mean()\n",
        "            rolling_std = pd.Series(self.y_data).rolling(window=window_size).std()\n",
        "            return rolling_mean.iloc[-self.DISPLAY_WINDOW:], rolling_std.iloc[-self.DISPLAY_WINDOW:]\n",
        "        return None, None\n",
        "\n",
        "    def setup_callbacks(self):\n",
        "        @self.app.callback(\n",
        "            [Output('live-graph', 'figure'),\n",
        "             Output('stats-graph', 'figure'),\n",
        "             Output('demand-with-predictions', 'figure'),\n",
        "             Output('percentage-error-graph', 'figure'),\n",
        "             Output('pre-trained-deviation', 'children'),\n",
        "             Output('online-deviation', 'children')],\n",
        "            [Input('graph-update', 'n_intervals')]\n",
        "        )\n",
        "        def update_graphs(n):\n",
        "\n",
        "            if not self.x_data or not self.y_data:\n",
        "                return go.Figure(), go.Figure(), go.Figure()\n",
        "\n",
        "            main_fig = go.Figure()\n",
        "\n",
        "            main_fig.add_trace(go.Scatter(\n",
        "                x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                y=self.y_data[-self.DISPLAY_WINDOW:],\n",
        "                mode='lines+markers',\n",
        "                name='Demand',\n",
        "                line=dict(color='#2980b9')\n",
        "            ))\n",
        "\n",
        "            y_min = min(self.y_data[-self.DISPLAY_WINDOW:]) if self.y_data else 0\n",
        "            y_max = max(self.y_data[-self.DISPLAY_WINDOW:]) if self.y_data else 1\n",
        "\n",
        "            for drift_point in self.drift_points:\n",
        "                if drift_point >= min(self.x_data[-self.DISPLAY_WINDOW:]) and \\\n",
        "                   drift_point <= max(self.x_data[-self.DISPLAY_WINDOW:]):\n",
        "\n",
        "                    line_color = \"purple\" if drift_point in self.drift_points_ks and drift_point in self.drift_points_wDistance else \\\n",
        "                                \"blue\" if drift_point in self.drift_points_ks else \\\n",
        "                                \"green\" if drift_point in self.drift_points_wDistance else \\\n",
        "                                \"red\"\n",
        "                    line_dash = \"dashdot\" if drift_point in self.drift_points_ks and drift_point in self.drift_points_wDistance else \\\n",
        "                               \"dot\" if drift_point in self.drift_points_ks else \\\n",
        "                               \"solid\" if drift_point in self.drift_points_wDistance else \\\n",
        "                               \"dash\"\n",
        "\n",
        "                    main_fig.add_shape(\n",
        "                        type=\"line\",\n",
        "                        x0=drift_point,\n",
        "                        x1=drift_point,\n",
        "                        y0=y_min,\n",
        "                        y1=y_max,\n",
        "                        line=dict(\n",
        "                            color=line_color,\n",
        "                            width=2,\n",
        "                            dash=line_dash,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            main_fig.update_layout(\n",
        "                title='Real-time Electricity Demand with Drift Detection',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Demand (MW)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.y_data[-50:]) * 0.95,\n",
        "                          max(self.y_data[-50:]) * 1.05]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.x_data[-50:]),\n",
        "                          max(self.x_data[-50:])]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            stats_fig = go.Figure()\n",
        "            rolling_mean, rolling_std = self.calculate_statistics()\n",
        "\n",
        "            if rolling_mean is not None and rolling_std is not None:\n",
        "                stats_fig.add_trace(go.Scatter(\n",
        "                    x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=rolling_mean,\n",
        "                    mode='lines',\n",
        "                    name='Moving Average',\n",
        "                    line=dict(color='#27ae60')\n",
        "                ))\n",
        "\n",
        "                stats_fig.add_trace(go.Scatter(\n",
        "                    x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=rolling_mean + rolling_std,\n",
        "                    mode='lines',\n",
        "                    name='Upper Bound',\n",
        "                    line=dict(color='#e74c3c', dash='dash')\n",
        "                ))\n",
        "\n",
        "                stats_fig.add_trace(go.Scatter(\n",
        "                    x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=rolling_mean - rolling_std,\n",
        "                    mode='lines',\n",
        "                    name='Lower Bound',\n",
        "                    line=dict(color='#e74c3c', dash='dash'),\n",
        "                    fill='tonexty'\n",
        "                ))\n",
        "\n",
        "            stats_fig.update_layout(\n",
        "                title='Moving Average and Volatility',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Demand (MW)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.y_data[-50:]) * 0.95,\n",
        "                          max(self.y_data[-50:]) * 1.05]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.x_data[-50:]),\n",
        "                          max(self.x_data[-50:])]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            predictions_fig = go.Figure()\n",
        "\n",
        "            predictions_fig.add_trace(go.Scatter(\n",
        "                x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                y=self.y_data[-self.DISPLAY_WINDOW:],\n",
        "                mode='lines',\n",
        "                name='Actual Demand',\n",
        "                line=dict(color='#3498db')\n",
        "            ))\n",
        "\n",
        "            if self.y_pre_trained_pred:\n",
        "                predictions_fig.add_trace(go.Scatter(\n",
        "                    x=[_ - 1 for _ in self.x_data[-self.DISPLAY_WINDOW:]],\n",
        "                    y=self.y_pre_trained_pred[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Predicted Demand for Pre Trained Model',\n",
        "                    line=dict(color='#e6ee12', dash='dash')\n",
        "                ))\n",
        "            if self.y_online_pred:\n",
        "                predictions_fig.add_trace(go.Scatter(\n",
        "                    x=[_ - 1 for _ in self.x_data[-self.DISPLAY_WINDOW:]],\n",
        "                    y=self.y_online_pred[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Predicted Demand for Online Learning Model',\n",
        "                    line=dict(color='#e67e22', dash='dash')\n",
        "                ))\n",
        "\n",
        "            predictions_fig.update_layout(\n",
        "                title='Electricity Demand and Predictions',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Demand (MW)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.y_data[-50:]) * 0.95,\n",
        "                          max(self.y_data[-50:]) * 1.05]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.x_data[-50:]),\n",
        "                          max(self.x_data[-50:])]\n",
        "                )\n",
        "            )\n",
        "            global pre_trained_sum, pre_trained_cnt, online_sum, online_cnt\n",
        "            pre_train = (pre_trained_sum / pre_trained_cnt)\n",
        "            online_train = (online_sum / online_cnt)\n",
        "            pe_fig = go.Figure()\n",
        "\n",
        "            if self.y_pre_trained_pe:\n",
        "                pe_fig.add_trace(go.Scatter(\n",
        "                    x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=self.y_pre_trained_pe[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Pre-trained Model % Error',\n",
        "                    line=dict(color='#e6ee12', dash='dash')\n",
        "                ))\n",
        "\n",
        "            if self.y_online_pe:\n",
        "                pe_fig.add_trace(go.Scatter(\n",
        "                    x=self.x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=self.y_online_pe[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Online Learning Model % Error',\n",
        "                    line=dict(color='#e67e22', dash='dash')\n",
        "                ))\n",
        "\n",
        "            pe_fig.update_layout(\n",
        "                title='Percentage Error Comparison',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Percentage Error (%)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[0, 20]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(self.x_data[-self.DISPLAY_WINDOW:]),\n",
        "                           max(self.x_data[-self.DISPLAY_WINDOW:])]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return main_fig, stats_fig, predictions_fig, pe_fig, \\\n",
        "                   f\"Pre-trained Model Average Deviation: {pre_train:.5f}\", \\\n",
        "                   f\"Online Model Average Deviation: {online_train:.5f}\"\n",
        "\n",
        "    def run_server(self, debug=True, port=8052):\n",
        "        self.app.run_server(debug=debug, port=port)\n",
        "\n",
        "def main(spark_df):\n",
        "    assembler = VectorAssembler(inputCols=[\"nswdemand\",\"running_avg_nswdemand\",\"day\",\"period\"], outputCol=\"features\")\n",
        "    df_features = assembler.transform(df_processed.drop(\"vicprice\", \"vicdemand\", \"transfer\")).select(\"features\", \"next_day_nswdemand\")\n",
        "\n",
        "    online_model = SGDRegressor(loss = 'squared_error')\n",
        "    lr = SGDRegressor(loss = 'squared_error', alpha=0.005)\n",
        "    batch_X = df_features.select(\"features\").rdd.map(lambda row: row['features'].toArray()).collect()\n",
        "    batch_y = df_features.select(\"next_day_nswdemand\").rdd.map(lambda row: row['next_day_nswdemand']).collect()\n",
        "    online_model.fit(batch_X, batch_y)\n",
        "    lr.fit(batch_X,batch_y)\n",
        "    pre_trained_model = lr\n",
        "\n",
        "    producer = KafkaDataProducer(spark_df, pre_trained_model, online_model)\n",
        "    producer.start_producing()\n",
        "\n",
        "    dashboard = RealtimeDashboard(pre_trained_model, online_model)\n",
        "    dashboard.run_server()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(df_processed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yv6p6oIKYWRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df_processed = df.drop(\"vicprice\")\n",
        "\n",
        "# Create a running average column based on 'vicdemand'\n",
        "window_spec = Window.orderBy(\"day\").rowsBetween(-10, 0)  # Adjust window size as needed\n",
        "df_processed = df_processed.withColumn(\"running_avg_nswdemand\", F.avg(\"nswdemand\").over(window_spec))\n",
        "df_processed = df_processed.withColumn(\"running_avg_vicdemand\", F.avg(\"vicdemand\").over(window_spec))\n",
        "window_spec_next = Window.orderBy(\"day\")\n",
        "df_processed = df_processed.withColumn(\"next_day_nswdemand\", F.lead(\"nswdemand\", 1).over(window_spec_next))\n",
        "df_processed = df_processed.withColumn(\"next_day_vicdemand\", F.lead(\"vicdemand\", 1).over(window_spec_next))\n",
        "avg_nswdemand = df_processed.select(F.avg(\"nswdemand\")).first()[0]\n",
        "df_processed = df_processed.withColumn(\n",
        "    \"next_day_nswdemand\",\n",
        "    F.when(F.col(\"next_day_nswdemand\").isNull(), avg_nswdemand).otherwise(F.col(\"next_day_nswdemand\"))\n",
        ")\n",
        "noise_magnitude = 0.05 * avg_nswdemand\n",
        "df_processed = df_processed.withColumn(\n",
        "    \"nswdemand\",\n",
        "    F.col(\"nswdemand\") + (F.rand(seed=42) - 0.5) * 2 * noise_magnitude\n",
        ")\n",
        "avg_vicdemand = df_processed.select(F.avg(\"vicdemand\")).first()[0]\n",
        "df_processed = df_processed.withColumn(\n",
        "    \"next_day_vicdemand\",\n",
        "    F.when(F.col(\"next_day_vicdemand\").isNull(), avg_vicdemand).otherwise(F.col(\"next_day_vicdemand\"))\n",
        ")\n",
        "noise_magnitude_vic = 0.05 * avg_vicdemand\n",
        "df_processed = df_processed.withColumn(\n",
        "    \"vicdemand\",\n",
        "    F.col(\"vicdemand\") + (F.rand(seed=42) - 0.5) * 2 * noise_magnitude_vic\n",
        ")\n",
        "df_processed.show()"
      ],
      "metadata": {
        "id": "iWsluEUTYWBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "import plotly.graph_objs as go\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import threading\n",
        "import time\n",
        "from datetime import datetime\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "import tensorflow_io as tfio\n",
        "import numpy as np\n",
        "from scipy.stats import wasserstein_distance, ks_2samp\n",
        "import random\n",
        "\n",
        "pre_trained_cnt_nsw, pre_trained_sum_nsw = 1, 0\n",
        "online_cnt_nsw, online_sum_nsw = 1, 0\n",
        "pre_trained_cnt_vic, pre_trained_sum_vic = 1, 0\n",
        "online_cnt_vic, online_sum_vic = 1, 0\n",
        "\n",
        "KAFKA_TOPIC = 'electricity_demand'\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import Row\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "import json\n",
        "\n",
        "class KafkaDataProducer:\n",
        "    def __init__(self, spark_df, pre_trained_model_nsw, online_model_nsw,\n",
        "                 pre_trained_model_vic, online_model_vic):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
        "        )\n",
        "\n",
        "        self.data = spark_df.toPandas()\n",
        "        self.current_index = 0\n",
        "        self.pre_trained_model_nsw = pre_trained_model_nsw\n",
        "        self.online_model_nsw = online_model_nsw\n",
        "        self.pre_trained_model_vic = pre_trained_model_vic\n",
        "        self.online_model_vic = online_model_vic\n",
        "\n",
        "    def start_producing(self):\n",
        "        def produce_data():\n",
        "            global pre_trained_cnt_nsw, pre_trained_sum_nsw, online_cnt_nsw, online_sum_nsw, \\\n",
        "                   pre_trained_cnt_vic, pre_trained_sum_vic, online_cnt_vic, online_sum_vic\n",
        "            while self.current_index < len(self.data):\n",
        "                row = self.data.iloc[self.current_index]\n",
        "                last_row = row\n",
        "                if self.current_index != 0:\n",
        "                    last_row = self.data.iloc[self.current_index - 1]\n",
        "\n",
        "                # NSWDemand Features and Prediction\n",
        "                nsw_features_vector = Vectors.dense([\n",
        "                    float(last_row['nswdemand']),\n",
        "                    float(last_row['running_avg_nswdemand']),\n",
        "                    int(row['day']),\n",
        "                    float(row['period'])\n",
        "                ])\n",
        "\n",
        "                # VicDemand Features and Prediction\n",
        "                vic_features_vector = Vectors.dense([\n",
        "                    float(last_row['vicdemand']),\n",
        "                    float(last_row['running_avg_vicdemand']),\n",
        "                    int(row['day']),\n",
        "                    float(row['period'])\n",
        "                ])\n",
        "\n",
        "                # Prepare Pandas DataFrames\n",
        "                nsw_pandas_df = pd.DataFrame({\n",
        "                    'features': [nsw_features_vector],\n",
        "                    'next_day_nswdemand': [float(row['nswdemand'])]\n",
        "                })\n",
        "                vic_pandas_df = pd.DataFrame({\n",
        "                    'features': [vic_features_vector],\n",
        "                    'next_day_vicdemand': [float(row['vicdemand'])]\n",
        "                })\n",
        "\n",
        "                # Convert to Spark DataFrames\n",
        "                nsw_spark_df = spark.createDataFrame(nsw_pandas_df)\n",
        "                vic_spark_df = spark.createDataFrame(vic_pandas_df)\n",
        "\n",
        "                # Predict for both NSW and Victoria\n",
        "                nsw_batch_X = nsw_spark_df.select(\"features\").rdd.map(lambda row: row['features'].toArray()).collect()\n",
        "                vic_batch_X = vic_spark_df.select(\"features\").rdd.map(lambda row: row['features'].toArray()).collect()\n",
        "\n",
        "                nsw_pre_trained_prediction = self.pre_trained_model_nsw.predict(nsw_batch_X)[0]\n",
        "                nsw_online_prediction = self.online_model_nsw.predict(nsw_batch_X)[0]\n",
        "\n",
        "                vic_pre_trained_prediction = self.pre_trained_model_vic.predict(vic_batch_X)[0]\n",
        "                vic_online_prediction = self.online_model_vic.predict(vic_batch_X)[0]\n",
        "\n",
        "                # Online model partial fit\n",
        "                nsw_batch_y = nsw_spark_df.select(\"next_day_nswdemand\").rdd.map(lambda row: row['next_day_nswdemand']).collect()\n",
        "                vic_batch_y = vic_spark_df.select(\"next_day_vicdemand\").rdd.map(lambda row: row['next_day_vicdemand']).collect()\n",
        "\n",
        "                self.online_model_nsw.partial_fit(nsw_batch_X, nsw_batch_y)\n",
        "                self.online_model_vic.partial_fit(vic_batch_X, vic_batch_y)\n",
        "\n",
        "                # Track prediction errors (similar to before, but separate for each state)\n",
        "                if self.current_index != 0:\n",
        "                    # NSW tracking\n",
        "                    pre_trained_cnt_nsw += 1\n",
        "                    online_cnt_nsw += 1\n",
        "                    pre_trained_sum_nsw += abs(float(last_row['nswdemand']) - float(nsw_pre_trained_prediction))\n",
        "                    online_sum_nsw += abs(float(last_row['nswdemand']) - float(nsw_online_prediction))\n",
        "\n",
        "                    # Victoria tracking\n",
        "                    pre_trained_cnt_vic += 1\n",
        "                    online_cnt_vic += 1\n",
        "                    pre_trained_sum_vic += abs(float(last_row['vicdemand']) - float(vic_pre_trained_prediction))\n",
        "                    online_sum_vic += abs(float(last_row['vicdemand']) - float(vic_online_prediction))\n",
        "\n",
        "                # Prepare Kafka message with both cities' data\n",
        "                message = {\n",
        "                    'timestamp': self.current_index,\n",
        "                    'nsw_demand': float(row['nswdemand']),\n",
        "                    'nsw_pre_trained_prediction': float(nsw_pre_trained_prediction),\n",
        "                    'nsw_online_prediction': float(nsw_online_prediction),\n",
        "                    'vic_demand': float(row['vicdemand']),\n",
        "                    'vic_pre_trained_prediction': float(vic_pre_trained_prediction),\n",
        "                    'vic_online_prediction': float(vic_online_prediction)\n",
        "                }\n",
        "\n",
        "                self.producer.send(KAFKA_TOPIC, message)\n",
        "                self.current_index += 1\n",
        "                time.sleep(0.05 + random.randint(1,10) / 1000)\n",
        "\n",
        "            self.producer.flush()\n",
        "            self.current_index = 0\n",
        "\n",
        "        thread = threading.Thread(target=produce_data)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "def predict(model, df_batch):\n",
        "    predictions = model.transform(df_batch)\n",
        "    return predictions.select(\"next_day_nswdemand\", \"prediction\")\n",
        "\n",
        "class RealtimeDashboard:\n",
        "    def __init__(self, pre_trained_models, online_models):\n",
        "\n",
        "        self.x_data = {'NSW': [], 'VIC': []}\n",
        "        self.y_data = {'NSW': [], 'VIC': []}\n",
        "        self.y_pre_trained_pred = {'NSW': [], 'VIC': []}\n",
        "        self.y_online_pred = {'NSW': [], 'VIC': []}\n",
        "        self.y_pre_trained_pe = {'NSW': [], 'VIC': []}\n",
        "        self.y_online_pe = {'NSW': [], 'VIC': []}\n",
        "        self.predicted_demand = []\n",
        "        self.abs_diff_sum_pre_trained = 0\n",
        "        self.abs_diff_sum_online = 0\n",
        "        self.DISPLAY_WINDOW = 200\n",
        "        self.DRIFT_WINDOW = 20\n",
        "        self.DRIFT_THRESHOLD = 2.0\n",
        "        self.KS_PVALUE_THRESHOLD = 0.2\n",
        "        self.WASSERSTEIN_THRESHOLD = 0.2\n",
        "\n",
        "        self.drift_points = []\n",
        "        self.drift_points_ks = []\n",
        "        self.drift_points_wDistance = []\n",
        "\n",
        "        self.consumer = KafkaConsumer(\n",
        "            KAFKA_TOPIC,\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "            auto_offset_reset='earliest'\n",
        "        )\n",
        "\n",
        "        self.app = dash.Dash(__name__)\n",
        "        self.create_layout()\n",
        "        self.setup_callbacks()\n",
        "\n",
        "        self.start_consuming()\n",
        "\n",
        "    def start_consuming(self):\n",
        "        def consume_data():\n",
        "            previous_nswdemand = None\n",
        "            for message in self.consumer:\n",
        "                data = message.value\n",
        "\n",
        "                # Process NSW data\n",
        "                self.x_data['NSW'].append(data['timestamp'])\n",
        "                self.y_data['NSW'].append(data['nsw_demand'])\n",
        "                self.y_pre_trained_pred['NSW'].append(data['nsw_pre_trained_prediction'])\n",
        "                self.y_online_pred['NSW'].append(data['nsw_online_prediction'])\n",
        "\n",
        "                # Process Victoria data\n",
        "                self.x_data['VIC'].append(data['timestamp'])\n",
        "                self.y_data['VIC'].append(data['vic_demand'])\n",
        "                self.y_pre_trained_pred['VIC'].append(data['vic_pre_trained_prediction'])\n",
        "                self.y_online_pred['VIC'].append(data['vic_online_prediction'])\n",
        "\n",
        "                # Calculate Percentage Error for both cities\n",
        "                if len(self.y_data['NSW']) > 1:\n",
        "                    nsw_pre_trained_pe = abs((self.y_data['NSW'][-2] - data['nsw_pre_trained_prediction']) / self.y_data['NSW'][-2]) * 100\n",
        "                    nsw_online_pe = abs((self.y_data['NSW'][-2] - data['nsw_online_prediction']) / self.y_data['NSW'][-2]) * 100\n",
        "                    self.y_pre_trained_pe['NSW'].append(nsw_pre_trained_pe)\n",
        "                    self.y_online_pe['NSW'].append(nsw_online_pe)\n",
        "\n",
        "                if len(self.y_data['VIC']) > 1:\n",
        "                    vic_pre_trained_pe = abs((self.y_data['VIC'][-2] - data['vic_pre_trained_prediction']) / self.y_data['VIC'][-2]) * 100\n",
        "                    vic_online_pe = abs((self.y_data['VIC'][-2] - data['vic_online_prediction']) / self.y_data['VIC'][-2]) * 100\n",
        "                    self.y_pre_trained_pe['VIC'].append(vic_pre_trained_pe)\n",
        "                    self.y_online_pe['VIC'].append(vic_online_pe)\n",
        "\n",
        "                # Trim data to display window for both cities\n",
        "                for city in ['NSW', 'VIC']:\n",
        "                    if len(self.x_data[city]) > self.DISPLAY_WINDOW:\n",
        "                        self.x_data[city] = self.x_data[city][-self.DISPLAY_WINDOW:]\n",
        "                        self.y_data[city] = self.y_data[city][-self.DISPLAY_WINDOW:]\n",
        "                        self.y_pre_trained_pred[city] = self.y_pre_trained_pred[city][-self.DISPLAY_WINDOW:]\n",
        "                        self.y_online_pred[city] = self.y_online_pred[city][-self.DISPLAY_WINDOW:]\n",
        "                        self.y_pre_trained_pe[city] = self.y_pre_trained_pe[city][-self.DISPLAY_WINDOW:]\n",
        "                        self.y_online_pe[city] = self.y_online_pe[city][-self.DISPLAY_WINDOW:]\n",
        "\n",
        "        thread = threading.Thread(target=consume_data)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "\n",
        "    def create_layout(self):\n",
        "\n",
        "        self.app.layout = html.Div([\n",
        "            html.H1(\"Real-time Electricity Demand Dashboard\",\n",
        "                    style={'textAlign': 'center', 'color': '#2c3e50', 'marginBottom': 20}),\n",
        "\n",
        "            # Add dropdown for city selection\n",
        "            dcc.Dropdown(\n",
        "                id='city-dropdown',\n",
        "                options=[\n",
        "                    {'label': 'New South Wales', 'value': 'NSW'},\n",
        "                    {'label': 'Victoria', 'value': 'VIC'}\n",
        "                ],\n",
        "                value='NSW',  # Default to NSW\n",
        "                style={'width': '50%', 'margin': '0 auto'}\n",
        "            ),\n",
        "            html.H1(\"Real-time Electricity Demand Dashboard\",\n",
        "                    style={'textAlign': 'center', 'color': '#2c3e50', 'marginBottom': 20}),\n",
        "\n",
        "            html.Div([\n",
        "                dcc.Graph(id='live-graph', animate=True),\n",
        "                dcc.Graph(id='stats-graph', animate=True),\n",
        "            ], style={'display': 'flex', 'flexDirection': 'row'}),\n",
        "\n",
        "            html.Div([\n",
        "                dcc.Graph(id='demand-with-predictions', animate=True),\n",
        "            ], style={'marginTop': 20}),\n",
        "            html.Div([\n",
        "                dcc.Graph(id='percentage-error-graph', animate=True),\n",
        "            ], style={'marginTop': 20}),\n",
        "            html.Div([\n",
        "                html.P(id='pre-trained-deviation'),\n",
        "                html.P(id='online-deviation'),\n",
        "            ]),\n",
        "\n",
        "            dcc.Interval(\n",
        "                id='graph-update',\n",
        "                interval=1000,\n",
        "                n_intervals=0\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def detect_drift_ks(self):\n",
        "        if len(self.y_data) >= self.DRIFT_WINDOW * 2:\n",
        "\n",
        "            prev_window = self.y_data[-2*self.DRIFT_WINDOW:-self.DRIFT_WINDOW]\n",
        "            curr_window = self.y_data[-self.DRIFT_WINDOW:]\n",
        "\n",
        "            ks_stat, ks_p_value = ks_2samp(prev_window, curr_window)\n",
        "\n",
        "            drift_detected = False\n",
        "\n",
        "            if ks_p_value < self.KS_PVALUE_THRESHOLD:\n",
        "                drift_detected = True\n",
        "\n",
        "            if drift_detected:\n",
        "\n",
        "                if len(self.drift_points_ks) == 0 or abs(self.drift_points_ks[-1] - self.x_data[-1]) >= 20:\n",
        "                    self.drift_points_ks.append(self.x_data[-1])\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "    def detect_drift_wDistance(self):\n",
        "        if len(self.y_data) >= self.DRIFT_WINDOW * 2:\n",
        "\n",
        "            prev_window = self.y_data[-2*self.DRIFT_WINDOW:-self.DRIFT_WINDOW]\n",
        "            curr_window = self.y_data[-self.DRIFT_WINDOW:]\n",
        "\n",
        "            wasserstein_dist = wasserstein_distance(prev_window, curr_window)\n",
        "\n",
        "            drift_detected = False\n",
        "\n",
        "            if wasserstein_dist > self.WASSERSTEIN_THRESHOLD:\n",
        "                drift_detected = True\n",
        "\n",
        "            if drift_detected:\n",
        "\n",
        "                if len(self.drift_points_wDistance) == 0 or abs(self.drift_points_wDistance[-1] - self.x_data[-1]) >= 20:\n",
        "                    self.drift_points_wDistance.append(self.x_data[-1])\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def detect_drift(self):\n",
        "        if len(self.y_data) >= self.DRIFT_WINDOW * 2:\n",
        "            prev_window = self.y_data[-2*self.DRIFT_WINDOW:-self.DRIFT_WINDOW]\n",
        "            curr_window = self.y_data[-self.DRIFT_WINDOW:]\n",
        "\n",
        "            wasserstein_dist = wasserstein_distance(prev_window, curr_window)\n",
        "            ks_stat, ks_p_value = ks_2samp(prev_window, curr_window)\n",
        "\n",
        "            drift_detected = False\n",
        "            if wasserstein_dist > self.WASSERSTEIN_THRESHOLD:\n",
        "                drift_detected = True\n",
        "            if ks_p_value < self.KS_PVALUE_THRESHOLD:\n",
        "                drift_detected = True\n",
        "\n",
        "            if drift_detected:\n",
        "                if len(self.drift_points) == 0 or abs(self.drift_points[-1] - self.x_data[-1]) >= 20:\n",
        "                    self.drift_points.append(self.x_data[-1])\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def calculate_statistics(self,y_data):\n",
        "        if len(y_data) >= 10:\n",
        "            window_size = 10\n",
        "            rolling_mean = pd.Series(y_data).rolling(window=window_size).mean()\n",
        "            rolling_std = pd.Series(y_data).rolling(window=window_size).std()\n",
        "            return rolling_mean.iloc[-self.DISPLAY_WINDOW:], rolling_std.iloc[-self.DISPLAY_WINDOW:]\n",
        "        return None, None\n",
        "\n",
        "    def setup_callbacks(self):\n",
        "        @self.app.callback(\n",
        "            [Output('live-graph', 'figure'),\n",
        "             Output('stats-graph', 'figure'),\n",
        "             Output('demand-with-predictions', 'figure'),\n",
        "             Output('percentage-error-graph', 'figure'),\n",
        "             Output('pre-trained-deviation', 'children'),\n",
        "             Output('online-deviation', 'children')],\n",
        "            [Input('city-dropdown', 'value'),Input('graph-update', 'n_intervals')]\n",
        "        )\n",
        "        def update_graphs(selected_city, n):\n",
        "            x_data = self.x_data[selected_city][-self.DISPLAY_WINDOW:]\n",
        "            y_data = self.y_data[selected_city][-self.DISPLAY_WINDOW:]\n",
        "            y_pre_trained_pred = self.y_pre_trained_pred[selected_city][-self.DISPLAY_WINDOW:]\n",
        "            y_online_pred = self.y_online_pred[selected_city][-self.DISPLAY_WINDOW:]\n",
        "            y_pre_trained_pe = self.y_pre_trained_pe[selected_city][-self.DISPLAY_WINDOW:]\n",
        "            y_online_pe = self.y_online_pe[selected_city][-self.DISPLAY_WINDOW:]\n",
        "\n",
        "            if not x_data or not y_data:\n",
        "                return go.Figure(), go.Figure(), go.Figure(),go.Figure(),'',''\n",
        "\n",
        "            main_fig = go.Figure()\n",
        "\n",
        "            main_fig.add_trace(go.Scatter(\n",
        "                x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                y=y_data[-self.DISPLAY_WINDOW:],\n",
        "                mode='lines+markers',\n",
        "                name='Demand',\n",
        "                line=dict(color='#2980b9')\n",
        "            ))\n",
        "\n",
        "            y_min = min(y_data[-self.DISPLAY_WINDOW:]) if y_data else 0\n",
        "            y_max = max(y_data[-self.DISPLAY_WINDOW:]) if y_data else 1\n",
        "\n",
        "            for drift_point in self.drift_points:\n",
        "                if drift_point >= min(x_data[-self.DISPLAY_WINDOW:]) and \\\n",
        "                   drift_point <= max(x_data[-self.DISPLAY_WINDOW:]):\n",
        "\n",
        "                    line_color = \"purple\" if drift_point in self.drift_points_ks and drift_point in self.drift_points_wDistance else \\\n",
        "                                \"blue\" if drift_point in self.drift_points_ks else \\\n",
        "                                \"green\" if drift_point in self.drift_points_wDistance else \\\n",
        "                                \"red\"\n",
        "                    line_dash = \"dashdot\" if drift_point in self.drift_points_ks and drift_point in self.drift_points_wDistance else \\\n",
        "                               \"dot\" if drift_point in self.drift_points_ks else \\\n",
        "                               \"solid\" if drift_point in self.drift_points_wDistance else \\\n",
        "                               \"dash\"\n",
        "\n",
        "                    main_fig.add_shape(\n",
        "                        type=\"line\",\n",
        "                        x0=drift_point,\n",
        "                        x1=drift_point,\n",
        "                        y0=y_min,\n",
        "                        y1=y_max,\n",
        "                        line=dict(\n",
        "                            color=line_color,\n",
        "                            width=2,\n",
        "                            dash=line_dash,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            main_fig.update_layout(\n",
        "                title='Real-time Electricity Demand with Drift Detection',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Demand (MW)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(y_data[-50:]) * 0.95,\n",
        "                          max(y_data[-50:]) * 1.05]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(x_data[-50:]),\n",
        "                          max(x_data[-50:])]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            stats_fig = go.Figure()\n",
        "            rolling_mean, rolling_std = self.calculate_statistics(y_data)\n",
        "\n",
        "            if rolling_mean is not None and rolling_std is not None:\n",
        "                stats_fig.add_trace(go.Scatter(\n",
        "                    x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=rolling_mean,\n",
        "                    mode='lines',\n",
        "                    name='Moving Average',\n",
        "                    line=dict(color='#27ae60')\n",
        "                ))\n",
        "\n",
        "                stats_fig.add_trace(go.Scatter(\n",
        "                    x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=rolling_mean + rolling_std,\n",
        "                    mode='lines',\n",
        "                    name='Upper Bound',\n",
        "                    line=dict(color='#e74c3c', dash='dash')\n",
        "                ))\n",
        "\n",
        "                stats_fig.add_trace(go.Scatter(\n",
        "                    x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=rolling_mean - rolling_std,\n",
        "                    mode='lines',\n",
        "                    name='Lower Bound',\n",
        "                    line=dict(color='#e74c3c', dash='dash'),\n",
        "                    fill='tonexty'\n",
        "                ))\n",
        "\n",
        "            stats_fig.update_layout(\n",
        "                title='Moving Average and Volatility',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Demand (MW)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(y_data[-50:]) * 0.95,\n",
        "                          max(y_data[-50:]) * 1.05]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(x_data[-50:]),\n",
        "                          max(x_data[-50:])]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            predictions_fig = go.Figure()\n",
        "\n",
        "            predictions_fig.add_trace(go.Scatter(\n",
        "                x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                y=y_data[-self.DISPLAY_WINDOW:],\n",
        "                mode='lines',\n",
        "                name='Actual Demand',\n",
        "                line=dict(color='#3498db')\n",
        "            ))\n",
        "\n",
        "            if y_pre_trained_pred:\n",
        "                predictions_fig.add_trace(go.Scatter(\n",
        "                    x=[_ - 1 for _ in x_data[-self.DISPLAY_WINDOW:]],\n",
        "                    y=y_pre_trained_pred[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Predicted Demand for Pre Trained Model',\n",
        "                    line=dict(color='#e6ee12', dash='dash')\n",
        "                ))\n",
        "            if y_online_pred:\n",
        "                predictions_fig.add_trace(go.Scatter(\n",
        "                    x=[_ - 1 for _ in x_data[-self.DISPLAY_WINDOW:]],\n",
        "                    y=y_online_pred[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Predicted Demand for Online Learning Model',\n",
        "                    line=dict(color='#e67e22', dash='dash')\n",
        "                ))\n",
        "\n",
        "            predictions_fig.update_layout(\n",
        "                title='Electricity Demand and Predictions',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Demand (MW)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(y_data[-50:]) * 0.95,\n",
        "                          max(y_data[-50:]) * 1.05]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(x_data[-50:]),\n",
        "                          max(x_data[-50:])]\n",
        "                )\n",
        "            )\n",
        "            global pre_trained_sum_nsw, pre_trained_cnt_nsw, online_sum_nsw, online_cnt_nsw, \\\n",
        "               pre_trained_sum_vic, pre_trained_cnt_vic, online_sum_vic, online_cnt_vic\n",
        "\n",
        "            if selected_city == 'NSW':\n",
        "                pre_train = (pre_trained_sum_nsw / pre_trained_cnt_nsw)\n",
        "                online_train = (online_sum_nsw / online_cnt_nsw)\n",
        "            else:\n",
        "                pre_train = (pre_trained_sum_vic / pre_trained_cnt_vic)\n",
        "                online_train = (online_sum_vic / online_cnt_vic)\n",
        "\n",
        "            pe_fig = go.Figure()\n",
        "\n",
        "            if y_pre_trained_pe:\n",
        "                pe_fig.add_trace(go.Scatter(\n",
        "                    x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=y_pre_trained_pe[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Pre-trained Model % Error',\n",
        "                    line=dict(color='#e6ee12', dash='dash')\n",
        "                ))\n",
        "\n",
        "            if y_online_pe:\n",
        "                pe_fig.add_trace(go.Scatter(\n",
        "                    x=x_data[-self.DISPLAY_WINDOW:],\n",
        "                    y=y_online_pe[-self.DISPLAY_WINDOW:],\n",
        "                    mode='lines',\n",
        "                    name='Online Learning Model % Error',\n",
        "                    line=dict(color='#e67e22', dash='dash')\n",
        "                ))\n",
        "\n",
        "            pe_fig.update_layout(\n",
        "                title='Percentage Error Comparison',\n",
        "                xaxis_title='Time',\n",
        "                yaxis_title='Percentage Error (%)',\n",
        "                plot_bgcolor='white',\n",
        "                paper_bgcolor='white',\n",
        "                showlegend=True,\n",
        "                yaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[0, 20]\n",
        "                ),\n",
        "                xaxis=dict(\n",
        "                    gridcolor='#f0f0f0',\n",
        "                    range=[min(x_data[-self.DISPLAY_WINDOW:]),\n",
        "                           max(x_data[-self.DISPLAY_WINDOW:])]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return main_fig, stats_fig, predictions_fig, pe_fig, \\\n",
        "                   f\"Pre-trained Model Average Deviation: {pre_train:.5f}\", \\\n",
        "                   f\"Online Model Average Deviation: {online_train:.5f}\"\n",
        "\n",
        "    def run_server(self, debug=True, port=8052):\n",
        "        self.app.run_server(debug=debug, port=port)\n",
        "\n",
        "def main(spark_df):\n",
        "    assembler = VectorAssembler(inputCols=[\"nswdemand\",\"running_avg_nswdemand\",\"day\",\"period\"], outputCol=\"nsw_features\")\n",
        "    vic_assembler = VectorAssembler(inputCols=[\"vicdemand\",\"running_avg_vicdemand\",\"day\",\"period\"], outputCol=\"vic_features\")\n",
        "\n",
        "    # NSW Features\n",
        "    nsw_df_features = assembler.transform(df_processed).select(\"nsw_features\", \"next_day_nswdemand\")\n",
        "    nsw_batch_X = nsw_df_features.select(\"nsw_features\").rdd.map(lambda row: row['nsw_features'].toArray()).collect()\n",
        "    nsw_batch_y = nsw_df_features.select(\"next_day_nswdemand\").rdd.map(lambda row: row['next_day_nswdemand']).collect()\n",
        "\n",
        "    # Victoria Features\n",
        "    vic_df_features = vic_assembler.transform(df_processed).select(\"vic_features\", \"next_day_vicdemand\")\n",
        "    vic_batch_X = vic_df_features.select(\"vic_features\").rdd.map(lambda row: row['vic_features'].toArray()).collect()\n",
        "    vic_batch_y = vic_df_features.select(\"next_day_vicdemand\").rdd.map(lambda row: row['next_day_vicdemand']).collect()\n",
        "\n",
        "    # Create models for both states\n",
        "    online_model_nsw = SGDRegressor(loss='squared_error')\n",
        "    online_model_vic = SGDRegressor(loss='squared_error')\n",
        "\n",
        "    lr_nsw = SGDRegressor(loss='squared_error', alpha=0.005)\n",
        "    lr_vic = SGDRegressor(loss='squared_error', alpha=0.005)\n",
        "\n",
        "    # Fit models\n",
        "    online_model_nsw.fit(nsw_batch_X, nsw_batch_y)\n",
        "    lr_nsw.fit(nsw_batch_X, nsw_batch_y)\n",
        "\n",
        "    online_model_vic.fit(vic_batch_X, vic_batch_y)\n",
        "    lr_vic.fit(vic_batch_X, vic_batch_y)\n",
        "\n",
        "    # Create producer and dashboard\n",
        "    producer = KafkaDataProducer(\n",
        "        spark_df,\n",
        "        pre_trained_model_nsw=lr_nsw,\n",
        "        online_model_nsw=online_model_nsw,\n",
        "        pre_trained_model_vic=lr_vic,\n",
        "        online_model_vic=online_model_vic\n",
        "    )\n",
        "    producer.start_producing()\n",
        "\n",
        "    dashboard = RealtimeDashboard(\n",
        "        pre_trained_models={'NSW': lr_nsw, 'VIC': lr_vic},\n",
        "        online_models={'NSW': online_model_nsw, 'VIC': online_model_vic}\n",
        "    )\n",
        "    dashboard.run_server()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(df_processed)"
      ],
      "metadata": {
        "id": "whTmO5JC37-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_kYL_rE4B8E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}